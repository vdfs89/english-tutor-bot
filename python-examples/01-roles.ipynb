{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61074117-b1db-4278-ad9e-2960bb0aa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2fc9f-3831-4cb2-a7cd-8eff1ccbeb02",
   "metadata": {},
   "source": [
    "Compreendendo os Roles\n",
    "\n",
    "User: Mensagem do usuário\n",
    "Assistant: Resposta do modelo\n",
    "System: Intruções de comportamento para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f88379fa-7ae9-4f49-a696-fff40aa98348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM (Large Language Model) é um modelo de inteligência artificial treinado com enormes volumes de texto para entender e gerar linguagem natural. Ele usa redes neurais do tipo *transformer* e aprende padrões, gramática, fatos e até nuances de estilo a partir dos dados de treinamento. Com isso, consegue responder perguntas, resumir textos, traduzir, escrever códigos ou criar conteúdo de forma coerente e contextualizada. Quanto maior o número de parâmetros e a diversidade dos dados, mais amplo e preciso costuma ser o seu desempenho.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Me explique de forma reduzida o que são LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7bf06-1062-48ef-9ad2-33dbd8f42f8d",
   "metadata": {},
   "source": [
    "Definindo um comportamento para modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eaf08c0-60db-4f92-9b70-bf93f3087e84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LLMs: Modelos de Linguagem de Grande Escala**\n",
      "\n",
      "Um LLM (Large Language Model) é um tipo de modelo de inteligência artificial (IA) projetado para processar e entender linguagem natural. Ele é treinado com grandes conjuntos de dados textuais e utiliza algoritmos de aprendizado profundo para aprender padrões e relações linguísticas.\n",
      "\n",
      "**Características principais:**\n",
      "\n",
      "* **Treinamento com grandes conjuntos de dados**: LLMs são treinados com milhões ou bilhões de palavras e frases.\n",
      "* **Aprendizado profundo**: Utilizam técnicas de aprendizado profundo, como redes neurais, para aprender padrões linguísticos.\n",
      "* **Capacidade de gerar texto**: Podem gerar texto coerente e natural, respondendo a perguntas, completando frases, etc.\n",
      "\n",
      "**Exemplos de aplicações:**\n",
      "\n",
      "* Assistente virtual\n",
      "* Tradução automática\n",
      "* Resposta a perguntas\n",
      "* Geração de conteúdo\n",
      "\n",
      "Esses modelos têm mudado a forma como interagimos com a tecnologia e têm muitas aplicações práticas em áreas como a comunicação, a educação e a indústria.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "         {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Você é um especialista em IA que responde de forma clara e objetiva.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Me explique de forma reduzida o que são LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12aefc-72e1-4942-ae89-0b09ec6eb6ea",
   "metadata": {},
   "source": [
    "Controlando Resposta do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a7e928-a81f-4787-9d31-5b76f89a1fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Exemplos práticos de uso de LLMs (Modelos de Linguagem de Grande Escala)**  \n",
      "\n",
      "| Área | Aplicação concreta | Benefício principal |\n",
      "|------|--------------------|---------------------|\n",
      "| **Atendimento ao cliente** | Chatbots e assistentes virtuais 24/7 | Reduz tempo de espera e custos operacionais |\n",
      "| **Suporte interno** | FAQ inteligente e busca de documentos corporativos | Acelera a resolução de dúvidas de colaboradores |\n",
      "| **Produção de conteúdo** | Redação automática de artigos, posts de blog, descrições de produtos | Escala a criação de texto sem perder qualidade |\n",
      "| **Tradução e localização** | Tradução quase‑instantânea de textos, legendas e interfaces | Expande alcance global de produtos e serviços |\n",
      "| **Resumidores** | Geração de resumos de relatórios, artigos científicos ou reuniões gravadas | Economiza tempo de leitura e facilita a tomada de decisão |\n",
      "| **Assistentes de programação** | Completação de código, geração de snippets e explicação de bugs | Aumenta a produtividade de desenvolvedores |\n",
      "| **Educação e tutoria** | Tutores personalizados que respondem dúvidas e criam exercícios | Personaliza o aprendizado e oferece feedback imediato |\n",
      "| **Análise de sentimentos** | Monitoramento de opiniões em redes sociais, avaliações de clientes | Ajuda a entender a percepção de marca em tempo real |\n",
      "| **Extração de dados** | Identificação de entidades, datas, valores em textos não estruturados | Automatiza a alimentação de bases de dados e relatórios |\n",
      "| **Recrutamento** | Triagem automática de currículos e geração de descrições de vagas | Agiliza o processo de seleção e diminui viés humano |\n",
      "\n",
      "Essas são apenas algumas das muitas formas como os LLMs podem ser integrados a produtos e processos para melhorar eficiência, reduzir custos e oferecer experiências mais inteligentes ao usuário.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Você é um especialista em IA que responde de forma clara e objetiva.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Me explique de forma reduzida o que são LLMs\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"LLMs com baixa latência permitem respostas mais rápidas, o que melhora a experiência do usuário.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Pode me dar exemplos de aplicações práticas?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c48031-b29a-4d8c-8b21-100f47d0de10",
   "metadata": {},
   "source": [
    "Usando System Prompt Refinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fea4b50-29f7-40cb-8470-5685b4789143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Resumo (até 3 linhas)**  \n",
      "LLMs com baixa latência respondem quase que instantaneamente, permitindo interações fluídas, decisões em tempo real e melhor experiência do usuário. Isso é crucial em aplicações críticas como assistentes de voz, suporte ao cliente e sistemas de controle automatizado.  \n",
      "\n",
      "---  \n",
      "\n",
      "## 1. O que é latência e por que “baixa latência” importa?  \n",
      "\n",
      "| Conceito | Definição prática | Impacto de alta latência |\n",
      "|----------|-------------------|--------------------------|\n",
      "| **Latência** | Tempo decorrido entre a entrada do usuário (ex.: pergunta) e a resposta do modelo. Medida em milissegundos (ms). | Esperas perceptíveis (≥300 ms) causam frustração, interrupções no fluxo de trabalho e podem gerar perdas financeiras. |\n",
      "| **Baixa latência** | Respostas entregues em < 100 ms (idealmente < 50 ms). | Interação quase em tempo real, sensação de “conversa natural”. |\n",
      "\n",
      "**Analogia simples:** imagine conversar com alguém por walkie‑talkie. Se houver um atraso de alguns segundos, a conversa se torna desconexa. Quando o atraso é quase nulo, a comunicação flui como se fosse cara‑a‑cara. O mesmo acontece entre humanos e LLMs.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Por que a baixa latência é crítica em diferentes setores  \n",
      "\n",
      "### 2.1 Assistentes de voz e chatbots  \n",
      "- **Experiência do usuário:** usuários esperam respostas quase imediatas ao dizer “Ok, Alexa…”. Um atraso > 200 ms já é percebido como lento.  \n",
      "- **Retenção:** Estudos mostram que cada segundo extra de espera pode reduzir a taxa de conversão em até 7 %.  \n",
      "\n",
      "### 2.2 Suporte ao cliente em tempo real  \n",
      "- **Resolução de problemas:** Em centros de contato, agentes assistidos por LLMs precisam de sugestões instantâneas; atrasos podem prolongar chamadas e aumentar custos.  \n",
      "- **SLA (Service Level Agreement):** Muitas empresas têm metas de tempo de resposta (< 2 s). A latência do modelo compõe parte desse SLA.\n",
      "\n",
      "### 2.3 Tradução simultânea e legendagem ao vivo  \n",
      "- **Eventos ao vivo:** Conferências, transmissões esportivas ou aulas online exigem que a tradução chegue quase simultaneamente ao áudio original.  \n",
      "- **Segurança:** Em sistemas de monitoramento multilíngue (ex.: comando de centrais de emergência), atrasos podem comprometer a tomada de decisão.\n",
      "\n",
      "### 2.4 Sistemas de controle autônomo  \n",
      "- **Veículos autônomos & robótica:** Decisões de navegação precisam ser tomadas em milissegundos; um LLM que sugere a melhor ação (ex.: “frear”) deve responder imediatamente.  \n",
      "- **Financeiro (trading algorítmico):** Estratégias baseadas em linguagem (análise de notícias) precisam gerar sinais antes que o mercado se mova.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Desafios técnicos para alcançar baixa latência  \n",
      "\n",
      "1. **Tamanho do modelo** – Modelos com bilhões de parâmetros são poderosos, mas requerem mais ciclos de computação.  \n",
      "2. **Infraestrutura de hardware** – GPUs/TPUs de alta performance, mas o custo e a disponibilidade podem ser limitantes.  \n",
      "3. **Comunicação de rede** – Latência de transmissão (round‑trip) entre cliente e servidor pode ser maior que o tempo de inferência.  \n",
      "4. **Gerenciamento de carga** – Picos de requisições podem sobrecarregar servidores, aumentando a fila de espera.  \n",
      "\n",
      "---\n",
      "\n",
      "## 4. Estratégias para reduzir a latência  \n",
      "\n",
      "| Estratégia | Como funciona | Quando aplicar |\n",
      "|------------|---------------|----------------|\n",
      "| **Quantização** | Reduz a precisão dos pesos (ex.: 16 bit → 8 bit), diminuindo o número de operações. | Quando a perda de qualidade é aceitável (< 1 % de degradação). |\n",
      "| **Pruning (poda)** | Remove neurônios ou conexões pouco relevantes, diminuindo o número total de parâmetros. | Modelos muito grandes que podem ser “espremitados”. |\n",
      "| **Distilação** | Treina um modelo menor (estudante) a imitar um grande (professor). | Quando se quer manter a performance geral, mas com menos recursos. |\n",
      "| **Modelos híbridos** | Combina um modelo “rápido” para consultas simples e um “pesado” para casos complexos. | Sistemas que atendem a diferentes níveis de complexidade. |\n",
      "| **Cache de respostas** | Armazena respostas frequentes (ex.: FAQs) para servir instantaneamente. | Perguntas repetitivas ou padrões conhecidos. |\n",
      "| **Edge inference** | Executa o modelo próximo ao usuário (dispositivo local ou edge server). | Aplicações móveis, IoT, ou quando a rede tem alta latência. |\n",
      "| **Batching inteligente** | Agrupa requisições em lotes pequenos para otimizar o uso da GPU sem introduzir espera significativa. | Serviços com tráfego moderado‑alto. |\n",
      "| **Optimização de software** | Uso de bibliotecas de inferência acelerada (e.g., TensorRT, ONNX Runtime) e kernels específicos. | Sempre recomendável para extrair o máximo do hardware. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Casos reais de sucesso  \n",
      "\n",
      "| Empresa / Produto | Estratégia de latência | Resultado |\n",
      "|-------------------|------------------------|-----------|\n",
      "| **OpenAI (ChatGPT)** | Modelos distribuídos em datacenters dedicados, quantização parcial e cache de prompts frequentes. | Respostas típicas < 150 ms para queries curtas. |\n",
      "| **Google Duplex** | Inferência na borda (nos servidores de data center mais próximos) + modelo de tamanho reduzido para chamadas curtas. | Conversas telefônicas indistinguíveis de humanos, sem percepções de atraso. |\n",
      "| **Microsoft Azure Cognitive Services – Speech Translation** | Uso de pipelines de codificação paralela + otimizações de hardware ASIC. | Tradução simultânea < 100 ms de latência de áudio‑texto. |\n",
      "| **Nvidia Riva** | Inferência em GPUs Tensor Core com TensorRT, quantização 8 bit. | Assistentes de voz corporativos entregando respostas em ~ 30 ms. |\n",
      "| **Shopify (Assistente de suporte)** | Modelo de distilação + caching de respostas de suporte padrão. | Redução de tempo médio de resposta ao cliente de 1.2 s → 0.3 s. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Checklist rápido para quem está construindo um LLM de baixa latência  \n",
      "\n",
      "1. **Defina metas de latência** (ex.: < 100 ms para respostas curtas).  \n",
      "2. **Escolha a arquitetura** – prefira modelos otimizados para inferência (ex.: LLaMA‑2‑7B, Falcon‑7B).  \n",
      "3. **Aplique quantização e/ou distilação** logo na fase de treinamento.  \n",
      "4. **Teste em hardware real** (GPU/TPU, edge devices) usando ferramentas de profiling (ex.: Nsight, TensorBoard).  \n",
      "5. **Implemente caching** para consultas recorrentes.  \n",
      "6. **Monitore a latência em produção** (percentis 50, 90, 99) e ajuste o batching.  \n",
      "7. **Planeje escalabilidade** – auto‑scaling de pods, balanceamento de carga geográfico.  \n",
      "\n",
      "---\n",
      "\n",
      "## 7. Conclusão  \n",
      "\n",
      "A baixa latência em LLMs não é apenas um detalhe de performance; é um fator determinante para a usabilidade, eficiência operacional e competitividade de produtos baseados em IA. Ao combinar otimizações de modelo, hardware adequado e arquiteturas de deployment inteligentes, é possível entregar respostas quase em tempo real, abrir novos casos de uso (ex.: controle autônomo, tradução ao vivo) e melhorar significativamente a experiência do usuário. \n",
      "\n",
      "---  \n",
      "\n",
      "**Próximos passos sugeridos**  \n",
      "\n",
      "- **Auditoria de latência**: meça o tempo de resposta atual do seu serviço.  \n",
      "- **Pilotagem de quantização**: experimente 8‑bit usando ONNX Runtime e avalie a perda de qualidade.  \n",
      "- **Teste de edge inference**: implemente um protótipo em um dispositivo Raspberry Pi ou Jetson Nano para validar ganhos de rede.  \n",
      "\n",
      "Com esses passos, você estará no caminho certo para transformar seu LLM em um motor de resposta instantânea, pronto para os desafios do mundo real.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Você é um especialista em Inteligência Artificial focado em explicar conceitos\"\n",
    "                \"de forma clara, extruturada e prática. Sempre inicie suas respostas com um\"\n",
    "                \"resumo em até 3 linhas, depois detalhe com exmplos reais e aplicações no mundo \"\n",
    "                \"e, quando necessário,,, use analogias simples.\"\n",
    "                \"Se o usuário pedir código, forneça exemplos em Python bem comentados.\"\n",
    "                       ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explique a importância de LLMs com baixa latência.\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31448c26-6538-4772-81d1-5978269b6d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
